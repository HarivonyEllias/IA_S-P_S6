{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction generer mot de longueur aleatoire entre 1 et 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_binary_word(min_length=1, max_length=7):\n",
    "    # Générer une longueur aléatoire pour le mot binaire\n",
    "    length = random.randint(min_length, max_length)\n",
    "    # Générer un mot binaire de la longueur spécifiée\n",
    "    word = ''.join(random.choice('01') for _ in range(length))\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction generer language de nombre de mot aleatoire entre 1 et 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['010010', '101101', '0111']\n"
     ]
    }
   ],
   "source": [
    "def generate_random_language(min_words=1, max_words=10, min_length=1, max_length=7):\n",
    "    num_words = random.randint(min_words, max_words)\n",
    "    language = [generate_random_binary_word(min_length, max_length) for _ in range(num_words)]\n",
    "    return language\n",
    "\n",
    "print(generate_random_language())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction pour les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log2\n",
    "import numpy as np\n",
    "\n",
    "# Nombre de mots dans le langage\n",
    "def number_of_words(language):\n",
    "    return len(language)\n",
    "\n",
    "# Longueur moyenne des mots\n",
    "def average_word_length(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    return np.mean([len(word) for word in language])\n",
    "\n",
    "# Longueur minimale des mots\n",
    "def min_word_length(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    return min([len(word) for word in language])\n",
    "\n",
    "# Longueur maximale des mots\n",
    "def max_word_length(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    return max([len(word) for word in language])\n",
    "\n",
    "# Distribution des longueurs des mots\n",
    "def word_length_distribution(language, max_length=7):\n",
    "    distribution = [0] * (max_length + 1)\n",
    "    for word in language:\n",
    "        distribution[len(word)] += 1\n",
    "    return distribution\n",
    "\n",
    "# Complexité de Shannon (entropie de Shannon)\n",
    "def shannon_entropy(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    concatenated = ''.join(language)\n",
    "    frequency = Counter(concatenated)\n",
    "    total_chars = len(concatenated)\n",
    "    entropy = -sum((count / total_chars) * log2(count / total_chars) for count in frequency.values())\n",
    "    return entropy\n",
    "\n",
    "# Redondance (nombre de mots qui sont des préfixes ou des suffixes d'autres mots)\n",
    "def redundancy(language):\n",
    "    redundancy_count = 0\n",
    "    for word in language:\n",
    "        for other_word in language:\n",
    "            if word != other_word and (word.startswith(other_word) or word.endswith(other_word)):\n",
    "                redundancy_count += 1\n",
    "                break\n",
    "    return redundancy_count\n",
    "\n",
    "# Proportion de bits 0 et 1\n",
    "def bit_proportion(language):\n",
    "    if len(language) == 0:\n",
    "        return (0, 0)\n",
    "    concatenated = ''.join(language)\n",
    "    total_bits = len(concatenated)\n",
    "    count_0 = concatenated.count('0')\n",
    "    count_1 = concatenated.count('1')\n",
    "    return (count_0 / total_bits, count_1 / total_bits)\n",
    "\n",
    "def lexical_diversity(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    unique_words = set(language)\n",
    "    return len(unique_words) / len(language)\n",
    "\n",
    "\n",
    "def average_word_frequency(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    word_counts = Counter(language)\n",
    "    return np.mean(list(word_counts.values()))\n",
    "\n",
    "def longest_common_prefix(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    if len(language) == 1:\n",
    "        return len(language[0])\n",
    "\n",
    "    # Sort the list and compare the first and last word\n",
    "    sorted_words = sorted(language)\n",
    "    first_word = sorted_words[0]\n",
    "    last_word = sorted_words[-1]\n",
    "    i = 0\n",
    "\n",
    "    # Find the common prefix between the first and last word\n",
    "    while i < len(first_word) and i < len(last_word) and first_word[i] == last_word[i]:\n",
    "        i += 1\n",
    "\n",
    "    return i\n",
    "\n",
    "def longest_common_suffix(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    if len(language) == 1:\n",
    "        return len(language[0])\n",
    "\n",
    "    # Reverse the words in the language\n",
    "    reversed_words = [word[::-1] for word in language]\n",
    "    # Sort the reversed words and compare the first and last reversed word\n",
    "    sorted_reversed_words = sorted(reversed_words)\n",
    "    first_word = sorted_reversed_words[0]\n",
    "    last_word = sorted_reversed_words[-1]\n",
    "    i = 0\n",
    "\n",
    "    # Find the common suffix by comparing the reversed words\n",
    "    while i < len(first_word) and i < len(last_word) and first_word[i] == last_word[i]:\n",
    "        i += 1\n",
    "\n",
    "    return i\n",
    "\n",
    "def number_of_unique_characters(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    concatenated = ''.join(language)\n",
    "    unique_chars = set(concatenated)\n",
    "    return len(unique_chars)\n",
    "\n",
    "def average_character_frequency(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    concatenated = ''.join(language)\n",
    "    char_counts = Counter(concatenated)\n",
    "    return np.mean(list(char_counts.values()))\n",
    "\n",
    "def character_entropy(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    concatenated = ''.join(language)\n",
    "    frequency = Counter(concatenated)\n",
    "    total_chars = len(concatenated)\n",
    "    entropy = -sum((count / total_chars) * log2(count / total_chars) for count in frequency.values())\n",
    "    return entropy\n",
    "\n",
    "def average_word_symmetry(language):\n",
    "    if len(language) == 0:\n",
    "        return 0\n",
    "    symmetry_scores = [sum(1 for a, b in zip(word, word[::-1]) if a == b) / len(word) for word in language]\n",
    "    return np.mean(symmetry_scores)\n",
    "\n",
    "def binary_run_length_distribution(language):\n",
    "    if len(language) == 0:\n",
    "        return {}\n",
    "    concatenated = ''.join(language)\n",
    "    run_lengths = {'0': [], '1': []}\n",
    "    current_char = concatenated[0]\n",
    "    current_run_length = 1\n",
    "    for char in concatenated[1:]:\n",
    "        if char == current_char:\n",
    "            current_run_length += 1\n",
    "        else:\n",
    "            run_lengths[current_char].append(current_run_length)\n",
    "            current_char = char\n",
    "            current_run_length = 1\n",
    "    run_lengths[current_char].append(current_run_length)\n",
    "    return {k: np.mean(v) if v else 0 for k, v in run_lengths.items()}\n",
    "\n",
    "def average_hamming_distance(language):\n",
    "    if len(language) < 2:\n",
    "        return 0\n",
    "    def hamming_distance(s1, s2):\n",
    "        return sum(c1 != c2 for c1, c2 in zip(s1, s2)) + abs(len(s1) - len(s2))\n",
    "    \n",
    "    distances = []\n",
    "    for i in range(len(language)):\n",
    "        for j in range(i + 1, len(language)):\n",
    "            distances.append(hamming_distance(language[i], language[j]))\n",
    "    \n",
    "    return np.mean(distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithme de Sardinas et Patterson\n",
    "Voici une implémentation simplifiée de l'algorithme de Sardinas et Patterson pour vérifier si un langage est un code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever le prefixe et s'il est vide on remplace par e\n",
    "def removeprefixe(language, prefixe):\n",
    "    result = [\n",
    "        (str[len(prefixe):] if str.startswith(prefixe) else str) or \"e\"\n",
    "        for str in language\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "# Enlever le epsilone\n",
    "def removeepsilone(language):\n",
    "    result = [\n",
    "        str[1:] if str.startswith(\"e\") else str\n",
    "        for str in language\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "# Residuel\n",
    "def residuel(language, mot):\n",
    "    residuel = [s for s in language if s.startswith(mot)]\n",
    "    if not residuel:\n",
    "        residuel.append(\"vide\")\n",
    "    residuel = removeprefixe(residuel, mot)\n",
    "    return residuel\n",
    "\n",
    "# Quotient c'est à dire l'union des languages \n",
    "def quotient(*arrays):\n",
    "    result_set = set()\n",
    "    for array in arrays:\n",
    "        for item in array:\n",
    "            if item != \"vide\": \n",
    "                result_set.add(item) \n",
    "    return sorted(result_set)\n",
    "\n",
    "# Vérifier si le mot est déjà passer\n",
    "def ifexist(residuel):\n",
    "    seen = set()\n",
    "    for r in residuel:\n",
    "        r_tuple = tuple(sorted(r))\n",
    "        if r_tuple in seen:\n",
    "            return True\n",
    "        seen.add(r_tuple)\n",
    "    return False\n",
    "\n",
    "# Enlever le string vide dans la language\n",
    "def residuel_func(language, mot):\n",
    "    residuel = [s for s in language if s.startswith(mot)]\n",
    "    if not residuel:\n",
    "        residuel.append(\"vide\")\n",
    "    return removeprefixe(residuel, mot)\n",
    "\n",
    "# Sardinas\n",
    "def sardinas(language):\n",
    "    residuel = []\n",
    "    L1 = []\n",
    "    for i in range(len(language)):\n",
    "        L11 = residuel_func(language, language[i])\n",
    "        L1.append(L11)\n",
    "\n",
    "    LUnion = quotient(*L1)\n",
    "    LUnion = removeepsilone(LUnion)\n",
    "\n",
    "    residuel.append(LUnion)\n",
    "\n",
    "    for i in range(len(residuel[0]) - 1, -1, -1):\n",
    "        if residuel[0][i] == \"\":\n",
    "            residuel[0].pop(i)\n",
    "\n",
    "    i = 0\n",
    "    while i < len(residuel):\n",
    "        if \"e\" not in residuel[i]:\n",
    "            residuelL = [residuel[i][j] for j in range(len(residuel[i]))]\n",
    "            firstL = []\n",
    "            secondL = []\n",
    "            for k in range(len(language)):\n",
    "                firstL.append(residuel_func(residuelL, language[k]))\n",
    "            for l in range(len(residuelL)):\n",
    "                secondL.append(residuel_func(language, residuelL[l]))\n",
    "            firstUnion = quotient(*firstL)\n",
    "            secondUnion = quotient(*secondL)\n",
    "            residuel.append(quotient(firstUnion, secondUnion))\n",
    "        i += 1\n",
    "        if ifexist(residuel):\n",
    "            break\n",
    "\n",
    "    return residuel\n",
    "\n",
    "# Code or not\n",
    "def verificationcode(language):\n",
    "    residuel = sardinas(language)\n",
    "    for res in residuel:\n",
    "        if \"e\" in res:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Vérifier si un langage est un code avec l'algorithme de Sardinas et Patterson\n",
    "def is_code(language):\n",
    "    return verificationcode(language)\n",
    "\n",
    "def generate_training_data(num_samples=5000):\n",
    "    data = []\n",
    "    num_codes = num_samples // 2\n",
    "    num_non_codes = num_samples - num_codes\n",
    "\n",
    "    codes_count = 0\n",
    "    non_codes_count = 0\n",
    "\n",
    "    while codes_count < num_codes or non_codes_count < num_non_codes:\n",
    "        language = generate_random_language()\n",
    "        is_language_code = is_code(language)\n",
    "        \n",
    "        if is_language_code and codes_count < num_codes:\n",
    "            features = {\n",
    "                'nombre_mots': number_of_words(language),\n",
    "                'longueur_moyenne': average_word_length(language),\n",
    "                'longueur_minimale': min_word_length(language),\n",
    "                'longueur_maximale': max_word_length(language),\n",
    "                'distribution_longueur': word_length_distribution(language),\n",
    "                'complexité_shannon': shannon_entropy(language),\n",
    "                'redondance': redundancy(language),\n",
    "                'proportion_0': bit_proportion(language)[0],\n",
    "                'proportion_1': bit_proportion(language)[1],\n",
    "                'entropie_des_caracteres': character_entropy(language),\n",
    "                'distance_de_hamming_moyenne': average_hamming_distance(language),\n",
    "                'est_code': is_language_code\n",
    "            }\n",
    "\n",
    "            data.append(features)\n",
    "            codes_count += 1\n",
    "        elif not is_language_code and non_codes_count < num_non_codes:\n",
    "            features = {\n",
    "                'nombre_mots': number_of_words(language),\n",
    "                'longueur_moyenne': average_word_length(language),\n",
    "                'longueur_minimale': min_word_length(language),\n",
    "                'longueur_maximale': max_word_length(language),\n",
    "                'distribution_longueur': word_length_distribution(language),\n",
    "                'complexité_shannon': shannon_entropy(language),\n",
    "                'redondance': redundancy(language),\n",
    "                'proportion_0': bit_proportion(language)[0],\n",
    "                'proportion_1': bit_proportion(language)[1],\n",
    "                'entropie_des_caracteres': character_entropy(language),\n",
    "                'distance_de_hamming_moyenne': average_hamming_distance(language),\n",
    "                'est_code': is_language_code\n",
    "            }\n",
    "            data.append(features)\n",
    "            non_codes_count += 1\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Générer les données et les enregistrer dans un fichier CSV\n",
    "training_data = generate_training_data(5000)  # 5000 samples as required\n",
    "training_data.to_csv('training_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
